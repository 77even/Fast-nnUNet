<div align="center">
  <img src="../public/fastnnunet-logo.png" alt="FastnnUNet Logo" width="600">
</div>

# âš™ï¸ FastnnUNet C++ Engine

The FastnnUNet C++ Engine is a high-performance implementation of the FastnnUNet architecture designed for production-level deployment. Built on CUDA operators and TensorRT, this engine provides ultra-fast inference for CT and MRI images, completing the entire inference pipeline in seconds.

## âœ¨ Key Features

- ğŸš€ **High Performance**: Optimized C++ implementation with CUDA operators and TensorRT acceleration
- ğŸ­ **Production Ready**: Engineered for stable clinical deployment with robust error handling
- âš¡ **Fast Inference**: Complete inference pipeline for 3D medical images in seconds
- ğŸ’¾ **Memory Efficient**: Optimized memory management for handling large volumetric data
- ğŸ–¥ï¸ **Platform Compatible**: Designed to work on various NVIDIA GPU platforms
- ğŸ¥ **Clinical Integration**: Easy integration with clinical workflows and DICOM systems

## ğŸ”§ Technical Specifications

- ğŸ’» **Implementation**: C++17 with CUDA 11.x/12.x support
- ğŸš„ **Acceleration**: TensorRT optimization with FP16/INT8 quantization options
- ğŸ“¦ **Dependencies**: CUDA, cuDNN, TensorRT, OpenCV (minimal)
- ğŸ® **GPU Support**: NVIDIA GPUs with compute capability 6.0+
- ğŸ“„ **Input Formats**: Supports NIfTI, DICOM, and raw data formats
- ğŸ”Œ **Integration**: C++ API available

## ğŸ“Š Performance Metrics

| Image Type | Size | Original nnUNet (PyTorch) | FastnnUNet (PyTorch) | FastnnUNet (C++/TensorRT) |
|------------|------|--------------------------|---------------------|--------------------------|
| Brain MRI  | 256Ã—256Ã—160 | 12-15s | 4-8s | 0.3-0.5s |
| Chest CT   | 512Ã—512Ã—400 | 30-40s | 14-16s | 0.1-0.5s |
| Abdominal CT | 512Ã—512Ã—500+ | 40-60s | 15-18s | 4.5-7.5s |

*All metrics measured on NVIDIA RTX 5070Ti 4090 4080 3080 3060 3050 2080Ti GPUs

## ğŸ“‹ Usage

### ğŸ’¼ C++ API Example

```cpp
#include "fastnnunet/engine.h"

// Initialize engine with model path
FastnnUNet::Engine engine("path/to/trt/model");

// Load and preprocess image
auto image = engine.loadImage("patient_ct.nii.gz/.nii");

// Run inference
auto segmentation = engine.infer(image);

// Save result
engine.saveSegmentation(segmentation, "output_segmentation.nii.gz");
```

## ğŸ› ï¸ Building from Source

Requirements:
- CUDA 11.x or newer
- TensorRT 8.x or newer
- CMake 3.18+
- GCC 7+ or MSVC 2022+

```bash
# Clone repository
git clone https://github.com/username/FastnnUNet.git
cd FastnnUNet/engine

# Build
mkdir build && cd build
cmake ..
make -j8

# Install
make install
```

## ğŸ“¤ Converting Models

The C++ engine uses optimized TensorRT models. Convert your ONNX models using:

```bash
# From ONNX
trtexec --onnx path/to/model.onnx --saveEngine engine.trt --fp16 --shapes=input:batch_size x 1 x D x H x W(enable batch infer)
```

## ğŸ“œ License

This component follows the same license as the main FastnnUNet project.

## ğŸ“ Citation

If you use the FastnnUNet C++ Engine in your research, please cite:

```
Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J., & Maier-Hein, K. H. (2021). 
nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. 
Nature methods, 18(2), 203-211.
``` 